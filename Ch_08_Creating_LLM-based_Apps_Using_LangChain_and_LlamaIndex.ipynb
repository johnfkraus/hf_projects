{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc633e1c-a235-4262-8ca0-45965367ca10",
   "metadata": {},
   "source": [
    "With LlamaIndex, you gain the ability to connect an LLM to your proprietary data, empowering it to address queries tailored to your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1758d643-b97e-4bcd-b87b-b5717f7c5369",
   "metadata": {},
   "source": [
    "A Large Language Model (LLM) is a type of AI model that is designed to understand and generate human-like text based on the patterns and structures it has learned from massive amount of training data.\n",
    "\n",
    "- Size and scale – LLMs are trained on massive amount of data, including books, articles, web sites, videos, and pictures (see Figure 8.1).\n",
    "- Pre-training – LLMs go through a pre-training phase where they make use of the large amount of training data to learn the statistical relationships between words and sentences. This allows them to acquire a general understanding of grammar, syntax, and semantics.\n",
    "- Fine-tuning – After the pre-training, LLMs are fine-tuned on specific tasks or datasets. This process makes the LLM perform specific tasks, such as text classification, sentiment analysis, language translation, etc.\n",
    "\n",
    "Trainable parameters are the variables within a machine learning or deep learning model that are adjusted during the training process to enable the model to make accurate predictions or perform a specific task. These parameters are learned from the training data and fine-tuned to optimize the model's performance on a given task.\n",
    "\n",
    "In neural networks, trainable parameters typically include:\n",
    "\n",
    "Weights – these are the values associated with the connections between neurons in different layers of the network. During training, the weights are repeatedly adjusted to allow the network to better learn the relationships between input data and the desired output.\n",
    "\n",
    "Biases – Biases are the values associated with each neuron in a neural network layer. They help shift the activation function's output and allow the network to learn complex patterns.\n",
    "\n",
    "In short, the more trainable parameters you have in a neural network, the longer it takes to train the system. Training a model with many trainable parameters requires immense computing power and memory. Hence, it is very expensive to train an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a110dd9-59c3-4559-85d6-5f65cb1e0755",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "LangChain is a framework built around LLMs , designed to simplify the creation of applications using LLMs. You can think of it as a “chain” that connects the various components that are required to create advanced use cases around LLMs. A chain may contain the following components:\n",
    "\n",
    "- Prompt Templates — templates for different types of conversations with LLMs\n",
    "- LLMs — Large Language Models such as GPT-3, GPT-4, etc\n",
    "- Agents — Agents use LLM to decide what actions to be taken\n",
    "- Memory — Short or Long term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf073957-0cd3-446a-adab-e63b57ec730f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain\n",
      "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.72 (from langchain)\n",
      "  Downloading langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting langsmith>=0.1.17 (from langchain)\n",
      "  Downloading langsmith-0.4.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain) (2.0.39)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.0.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.12.2)\n",
      "Requirement already satisfied: packaging>=23.2 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain)\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/HuggingFaceBook/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.0)\n",
      "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading langsmith-0.4.13-py3-none-any.whl (372 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.7/633.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: zstandard, jsonpointer, requests-toolbelt, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [langchain]━\u001b[0m \u001b[32m7/8\u001b[0m [langchain]core]\n",
      "\u001b[1A\u001b[2KSuccessfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-core-0.3.74 langchain-text-splitters-0.3.9 langsmith-0.4.13 requests-toolbelt-1.0.0 zstandard-0.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a73afce-6579-4228-ace1-087edf276b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\nQuestion: {question}\\nAnswer: \\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    " \n",
    "template = '''\n",
    "Question: {question}\n",
    "Answer: \n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = template,\n",
    "    input_variables = ['question']\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8311b0d-050e-4781-8bf7-f4cfb0d429a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\nQuestion: {question}\\nAnswer: ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PromptTemplate(input_variables=['question'], \n",
    "               template='\\nQuestion: {question}\\nAnswer: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8462dc14-dec8-4ba9-ac46-c06e5af22164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'Your_HuggingFace_Token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148aa75e-6b9f-449f-8691-aed6d6d32dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
